---
title: "Case Study: Bellabeat"
output: html_document
---

## Business task

Bellabeat is a high-tech manufacturer of health-focused products for women.
Their Chief Creative Officer and cofunder (Urška Sršen) believes that analyzing
smart device fitness data could help unlock new growth opportunities for the
company. Sršen asks you to analyze smart device usage data in order to gain
insight into how customers use non-Bellabeat smart devices and then select one
Bellabeat product to apply these insights, considering:

1. What are some trends in smart device usage?
2. How could these trends apply to Bellabeat customers?
3. How could these trends help influence Bellabeat marketing strategy?

## Gathering data

Data collected from Kaggle's [**FitBit Fitness Tracker Data**](https://www.kaggle.com/arashnic/fitbit) dataset

**Content**
These datasets were generated by respondents to a distributed survey via Amazon
Mechanical Turk between 03.12.2016-05.12.2016. Thirty eligible Fitbit users
consented to the submission of personal tracker data, including minute-level
output for physical activity, heart rate, and sllep monitoring. Individual
reports can be parsed by export session ID (column A) or timestamp (column B).
Variation between output represents use of different types of Fitbit trackers
and individual tracking behaviors / preferences.

## Libraries
```{r Libraries, include=FALSE}
library(tidyverse)
library(waldo)  # compare()
library(lubridate)
library(magrittr)  # %<>%
library(skimr)  # skim() to get a good summary
```


## Understanding data

All data files obtained from Kaggle are stored in the *data/original data*
folder.
```{r Files list}
list.files(path = "../data/original_data")
```

Let's start with *dailyActivity_merged.csv* file:
```{r}
daily_activity <- read_csv("../data/original_data/dailyActivity_merged.csv")
```

Although the content description says that the data is for thirty users in the
date range from March 12 to May 12 of 2016, in the data files I have found data
for 33 different users in the data range from April 12 to May 12 of 2016, so I
will assume that the content description is wrong.
```{r}
unique(daily_activity$Id)
unique(daily_activity$ActivityDate)
```

Let's check daily files.
```{r}
daily_steps <- read_csv("../data/original_data/dailySteps_merged.csv")
daily_intensities <- read_csv("../data/original_data/dailyIntensities_merged.csv")
daily_calories <- read_csv("../data/original_data/dailyCalories_merged.csv")
daily_sleep <- read_csv("../data/original_data/sleepDay_merged.csv")
daily_weight <- read_csv("../data/original_data/weightLogInfo_merged.csv")
```

Some of the data is repeated over different files. Comparing columns with
waldo's compare() method allows me to dismiss some of those files.
```{r}
compare(daily_activity, daily_steps, max_diffs = Inf)
compare(daily_activity$ActivityDate, daily_steps$ActivityDay)
compare(daily_activity$TotalSteps, daily_steps$StepTotal)

compare(daily_activity, daily_intensities, max_diffs = Inf)
compare(daily_activity$ActivityDate, daily_intensities$ActivityDay)

compare(daily_activity, daily_calories, max_diffs = Inf)
compare(daily_activity$ActivityDate, daily_calories$ActivityDay)
```

So we can forget about *dailyCalories_merged.csv*, *dailyIntensities_merged.csv*
and *dailySteps_merged.csv*

*sleepDay_merged.csv* and *weightLogInfo_merged.csv* also contain daily data:
since the records correspond to a single time (for sleep data) and different
times but in single days (for weight data), I will convert the format to date
in order to merge all daily data.
```{r}
daily_activity %<>%
  mutate(ActivityDate = mdy(ActivityDate))
daily_sleep %<>%
  mutate(SleepDay = as_date(mdy_hms(SleepDay)))
daily_weight %<>%
  mutate(Date = as_date(mdy_hms(Date)))
```

Now I can merge all daily files into one:
```{r}
daily_data <- left_join(daily_activity, daily_sleep,
                        by = c("Id" = "Id", "ActivityDate" = "SleepDay")) %>%
  left_join(., daily_weight, by = c("Id" = "Id", "ActivityDate" = "Date"))
```

```{r}
daily_data
```

Now we have 943 rows (originally there were 940 rows), so let's see if any
duplicated row has appeared.
```{r}
compare(daily_activity$ActivityDate, daily_data$ActivityDate)
```

There are three duplicated rows (#437, #533 and # 835). Let's remove them.
```{r}
daily_data %>% distinct(.)
```

Let's save daily_data.
```{r}
write_csv(daily_data, '../data/daily_data.csv')
```

And then discard unnecessary objects.
```{r}
rm(daily_activity, daily_calories, daily_intensities, daily_sleep,
   daily_steps, daily_weight)
```

